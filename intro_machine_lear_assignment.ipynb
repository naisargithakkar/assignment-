{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b5b3d28-87b2-4da2-ad12-f2ba59b8d891",
   "metadata": {},
   "source": [
    "q1:\n",
    "    A statistical model or a machine learning algorithm is said to have underfitting when a model is too simple to capture data complexities. It represents the inability of the model to learn the training data effectively result in poor performance both on the training and testing data. In simple terms, an underfit model’s are inaccurate, especially when applied to new, unseen examples. It mainly happens when we uses very simple model with overly simplified assumptions. To address underfitting problem of the model, we need to use more complex models, with enhanced feature representation, and less regularization.\n",
    "    Reasons for Underfitting\n",
    "The model is too simple, So it may be not capable to represent the complexities in the data.\n",
    "The input features which is used to train the model is not the adequate representations of underlying factors influencing the target variable.\n",
    "The size of the training dataset used is not enough.\n",
    "Excessive regularization are used to prevent the overfitting, which constraint the model to capture the data well.\n",
    "Features are not scaled.\n",
    "Overfitting in Machine Learning\n",
    "A statistical model is said to be overfitted when the model does not make accurate predictions on testing data. When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data set. And when testing with test data results in High variance. Then the model does not categorize the data correctly, because of too many details and noise. The causes of overfitting are the non-parametric and non-linear methods because these types of machine learning algorithms have more freedom in building the model based on the dataset and therefore they can really build unrealistic models. A solution to avoid overfitting is using a linear algorithm if we have linear data or using the parameters like the maximal depth if we are using decision trees. \n",
    "Reasons for Overfitting:\n",
    " High variance and low bias.\n",
    "The model is too complex.\n",
    "The size of the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7cc0d9-7e9d-4eaa-a0b2-97518cfd80c8",
   "metadata": {},
   "source": [
    "q2:\n",
    "    Increase training data.\n",
    "Reduce model complexity.\n",
    "Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
    "Ridge Regularization and Lasso Regularization.\n",
    "Use dropout for neural networks to tackle overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268c9523-c945-4597-9d04-777e67c8fcc1",
   "metadata": {},
   "source": [
    "q3:\n",
    "    Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. To avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data. As a result, it may fail to find the best fit of the dominant trend in the data.\n",
    "    In the case of underfitting, the model is not able to learn enough from the training data, and hence it reduces the accuracy and produces unreliable predictions.\n",
    "\n",
    "An underfitted model has high bias and low variance.\n",
    "Reasons for Underfitting\n",
    "The model is too simple, So it may be not capable to represent the complexities in the data.\n",
    "The input features which is used to train the model is not the adequate representations of underlying factors influencing the target variable.\n",
    "The size of the training dataset used is not enough.\n",
    "Excessive regularization are used to prevent the overfitting, which constraint the model to capture the data well.\n",
    "Features are not scaled.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968fa56f-dd91-4f1e-9d63-cfe3558a9219",
   "metadata": {},
   "source": [
    "q4:\n",
    "    The bias is known as the difference between the prediction of the values by the Machine Learning model and the correct value. Being high in biasing gives a large error in training as well as testing data. It recommended that an algorithm should always be low-biased to avoid the problem of underfitting. By high bias, the data predicted is in a straight line format, thus not fitting accurately in the data in the data set. Such fitting is known as the Underfitting of Data. This happens when the hypothesis is too simple or linear in nature. Refer to the graph given below for an example of such a situation.\n",
    "    The variability of model prediction for a given data point which tells us the spread of our data is called the variance of the model. The model with high variance has a very complex fit to the training data and thus is not able to fit accurately on the data which it hasn’t seen before. As a result, such models perform very well on training data but have high error rates on test data. When a model is high on variance, it is then said to as Overfitting of Data. Overfitting is fitting the training set accurately via complex curve and high order hypothesis but is not the solution as the error with unseen data is high. While training a data model variance should be kept low. The high variance data looks as follows.\n",
    "    f the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as a Trade-off or Bias Variance Trade-off. This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time\n",
    "     We try to optimize the value of the total error for the model by using the Bias-Variance Tradeoff.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe80a7e2-13e2-49d9-967c-d4ad9b04a549",
   "metadata": {},
   "source": [
    "q5:\n",
    "    Ensure that you are using validation loss next to training loss in the training phase.\n",
    "When your validation loss is decreasing, the model is still underfit.\n",
    "When your validation loss is increasing, the model is overfit.\n",
    "When your validation loss is equal, the model is either perfectly fit or in a local minimum.\n",
    "Check the training and test/validation metrics and compare them. If the training metric is much better than the test/validation metric, then there are high chances you are overfitting.\n",
    "Plot learning curves to check for overfitting.\n",
    "Overfitting models produce good predictions for data points in the training set but perform poorly on new samples.\n",
    "Underfitting occurs when the machine learning model is not well-tuned to the training set. The resulting model is not capturing the relationship between input and output well enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e5fdcd-caff-4dab-aee0-54b5a5dae5a4",
   "metadata": {},
   "source": [
    "q6:\n",
    "    a machine learning model analyses the data, find patterns in it and make predictions. While training, the model learns these patterns in the dataset and applies them to test data for prediction. While making predictions, a difference occurs between prediction values made by the model and actual values/expected values, and this difference is known as bias errors or Errors due to bias. It can be defined as an inability of machine learning algorithms such as Linear Regression to capture the true relationship between the data points. Each algorithm begins with some amount of bias because bias occurs from assumptions in the model, which makes the target function simple to learn.\n",
    "    ow Bias: A low bias model will make fewer assumptions about the form of the target function.\n",
    "High Bias: A model with a high bias makes more assumptions, and the model becomes unable to capture the important features of our dataset. A high bias model also cannot perform well on new data.\n",
    "Generally, a linear algorithm has a high bias, as it makes them learn fast. The simpler the algorithm, the higher the bias it has likely to be introduced. Whereas a nonlinear algorithm often has low bias.\n",
    "\n",
    "Some examples of machine learning algorithms with low bias are Decision Trees, k-Nearest Neighbours and Support Vector Machines. At the same time, an algorithm with high bias is Linear Regression, Linear Discriminant Analysis and Logistic Regression.\n",
    "The variance would specify the amount of variation in the prediction if the different training data was used. In simple words, variance tells that how much a random variable is different from its expected value. Ideally, a model should not vary too much from one training dataset to another, which means the algorithm should be good in understanding the hidden mapping between inputs and output variables. Variance errors are either of low variance or high variance.\n",
    "\n",
    "Low variance means there is a small variation in the prediction of the target function with changes in the training data set. At the same time, High variance shows a large variation in the prediction of the target function with changes in the training dataset.\n",
    "\n",
    "A model that shows high variance learns a lot and perform well with the training dataset, and does not generalize well with the unseen dataset. As a result, such a model gives good results with the training dataset but shows high error rates on the test dataset.\n",
    "Since, with high variance, the model learns too much from the dataset, it leads to overfitting of the model. A model with high variance has the below problems:\n",
    "\n",
    "A high variance model leads to overfitting.\n",
    "Increase model complexities.\n",
    "Usually, nonlinear algorithms have a lot of flexibility to fit the model, have high variance\n",
    "Some examples of machine learning algorithms with low variance are, Linear Regression, Logistic Regression, and Linear discriminant analysis. At the same time, algorithms with high variance are decision tree, Support Vector Machine, and K-nearest neighbours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b221436-abca-4e03-9bf0-27e096bfb326",
   "metadata": {},
   "source": [
    "q7:\n",
    "    Regularization is a technique used to reduce errors by fitting the function appropriately on the given training set and avoiding overfitting. The commonly used regularization techniques are : \n",
    "\n",
    "Lasso Regularization – L1 Regularization\n",
    "Ridge Regularization – L2 Regularization\n",
    "Elastic Net Regularization – L1 and L2 Regularization\n",
    "A linear regression that uses the L2 regularization technique is called ridge regression. In other words, in ridge regression, a regularization term is added to the cost function of the linear regression, which keeps the magnitude of the model’s weights (coefficients) as small as possible. The L2 regularization technique tries to keep the model’s weights close to zero, but not zero, which means each feature should have a low impact on the output while the model's accuracy should be as high as possible.\n",
    "L1 Regularization\n",
    "Least Absolute Shrinkage and Selection Operator (lasso) regression is an alternative to ridge for regularizing linear regression. Lasso regression also adds a penalty term to the cost function, but slightly different, called L1 regularization. L1 regularization makes some coefficients zero, meaning the model will ignore those features. Ignoring the least important features helps emphasize the model's essential features.\n",
    "\n",
    "Elastic Net\n",
    "The Elastic Net is a regularized regression technique combining ridge and lasso's regularization terms. The \n",
    " parameter controls the combination ratio. When \n",
    ", the L2 term will be eliminated, and when \n",
    ", the L1 term will be removed.\n",
    "Although combining the penalties of lasso and ridge usually works better than only using one of the regularization techniques, adjusting two parameters, \n",
    " and \n",
    ", is a little tricky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbd96a6-822b-4ec9-a212-b578f69cc224",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
